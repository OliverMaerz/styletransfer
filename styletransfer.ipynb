{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Style transfer jupyter notebook based on the paper from LA Gatys et al.: \n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf \n",
    "\n",
    "This notebook uses PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure filenames, parameters etc.\n",
    "\n",
    "# Filenames of images to process \n",
    "# One of each must be in both the content_images and he style_images\n",
    "# The result will be stored using the same filename but in the output_images folder\n",
    "image_filenames = ['image1.jpg']\n",
    "\n",
    "# Paths for three types of images\n",
    "content_images = './content_images/'\n",
    "style_images = './style_images/'\n",
    "output_images = './output_images/'\n",
    "\n",
    "# Define weights for style layers\n",
    "style_weights = {'conv1_1': 1.,\n",
    "                 'conv2_1': 0.8,\n",
    "                 'conv3_1': 0.6,\n",
    "                 'conv4_1': 0.4,\n",
    "                 'conv5_1': 0.2}\n",
    "\n",
    "# Define content and style weight\n",
    "content_weight = 1\n",
    "style_weight = 1000000\n",
    "\n",
    "\n",
    "\n",
    "# Define the number of iterations for each image loop\n",
    "iterations = 5000\n",
    "\n",
    "\n",
    "\n",
    "# Image size to use\n",
    "output_max_size = 800\n",
    "# output size 600x600 uses appox. 3 GB memory in you GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU with CUDA is available and then load the VGG19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU w/CUDA is available else use CPU\n",
    "torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load pretrained VGG19 feature layers into GPU (or regular memory for CPU when no GPU w/CUDA is available)\n",
    "vgg19_features = models.vgg19(pretrained=True).features.to(torch_device)\n",
    "vgg19_features.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_loader(image_file):\n",
    "    \"\"\"Function to load content and style images, resize, convert to tensor, and normalize\"\"\"\n",
    "    content_image = Image.open(content_images + image_file).convert('RGB')\n",
    "    style_image = Image.open(style_images + image_file).convert('RGB')\n",
    "    \n",
    "    # define normalizer based based on mean and std defined here: \n",
    "    # https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "    # resize image \n",
    "    current_size = content_image.size\n",
    "    xy_ratio = current_size[0]/current_size[1]\n",
    "    output_size = list(current_size)\n",
    "    # print(xy_ratio)\n",
    "    if (max(current_size) > output_max_size):\n",
    "        if (output_max_size < 1):\n",
    "            output_size[0] = output_max_size\n",
    "            output_size[1] = int(output_max_size * xy_ratio)\n",
    "        else: \n",
    "            output_size[0] = int(output_max_size * xy_ratio)\n",
    "            output_size[1] = output_max_size \n",
    "    \n",
    "    # convert to tensor, normalize also remove alpha channel and add batch dimension\n",
    "    content_image = transform(content_image.resize(output_size, Image.ANTIALIAS))[:3,:,:].unsqueeze(0)\n",
    "    style_image = transform(style_image.resize(output_size, Image.ANTIALIAS))[:3,:,:].unsqueeze(0)\n",
    "    \n",
    "    return content_image, style_image\n",
    "\n",
    "\n",
    "def get_features(image, model):\n",
    "    \"\"\"Run image through the model\"\"\" \n",
    "    x = image\n",
    "    features = {}\n",
    "    # Layers for the content and style representations of image. For more details see \n",
    "    # https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n",
    "    style_layers = {'0': 'conv1_1',\n",
    "                    '5': 'conv2_1',\n",
    "                    '10': 'conv3_1',\n",
    "                    '19': 'conv4_1',\n",
    "                    '28': 'conv5_1'}\n",
    "    content_layers = {'21': 'conv4_2'}\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in style_layers:\n",
    "            features[style_layers[name]] = x\n",
    "        if name in content_layers:\n",
    "            features[content_layers[name]] = x           \n",
    "    return features\n",
    "\n",
    "\n",
    "def gram(x):\n",
    "    \"\"\"Computer gram matrix\"\"\"\n",
    "    # Compute gram matrix (matrix multiplied with its transpose matrix)\n",
    "    _, c, h, w = x.size()\n",
    "    x = x.view(c, h * w)\n",
    "    # multiply x with its transpose\n",
    "    x = torch.mm(x, x.t())\n",
    "    return x\n",
    "  \n",
    "    \n",
    "def convert_back_to_image(tensor, image_filename):\n",
    "    \"\"\"Convert tensor back to (PIL) image and save it on disk\"\"\"\n",
    "    # Move tensor to regular memory (CPU) etc.\n",
    "    image = tensor.to(\"cpu\").clone().detach().numpy().squeeze().transpose(1,2,0)\n",
    "    # Reverse the normalization applied earlier\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    # Get rid of values smaller than 0 and bigger than 1\n",
    "    image = image.clip(0, 1)\n",
    "    # Convert from array to image \n",
    "    image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "    # ... and save \n",
    "    image.save(image_filename)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through the images and create the new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: image1.jpg\n",
      "........"
     ]
    }
   ],
   "source": [
    "# print out a little warning when no GPU with CUDA present\n",
    "if torch_device == torch.device(\"cpu\"):\n",
    "    print('Running on CPU. This will take a while ...')\n",
    "\n",
    "    \n",
    "    \n",
    "# Iterate over the list of filenames\n",
    "for image_filename in image_filenames:\n",
    "    \n",
    "    print(f'Processing image: {image_filename}')\n",
    "    \n",
    "    content_image, style_image = images_loader(image_filename)\n",
    "    content_image = content_image.to(torch_device)\n",
    "    style_image = style_image.to(torch_device)\n",
    "    \n",
    "    # Content and style features\n",
    "    content_features = get_features(content_image, vgg19_features)\n",
    "    style_features = get_features(style_image, vgg19_features)\n",
    "    # Gram matrices \n",
    "    style_grams = {layer: gram(style_features[layer]) for layer in style_features}\n",
    "    \n",
    "    # Create output image as copy from content_image (as opposed to random, zeros etc.)\n",
    "    output_image = content_image.clone().requires_grad_(True).to(torch_device)\n",
    "\n",
    "    # Use Adam optimizer, set learing rate \n",
    "    optimizer = optim.Adam([output_image], lr=0.003)\n",
    "    \n",
    "    # Now loop x times (number of iteration defined by variable iteration above) over the model\n",
    "    for i in range(iterations):\n",
    "        output_features = get_features(output_image, vgg19_features)\n",
    "\n",
    "        # Content loss\n",
    "        content_loss = torch.mean((output_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "        #  Initialize the style loss with zero\n",
    "        style_loss = 0\n",
    "        # Iterate through layers and compute style loss\n",
    "        for layer in style_weights:\n",
    "            output_feature = output_features[layer]\n",
    "            _, c, h, w = output_feature.shape\n",
    "            output_gram = gram(output_feature)\n",
    "            style_gram = style_grams[layer]\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((output_gram - style_gram)**2)\n",
    "            style_loss += layer_style_loss / (c * h * w)\n",
    "\n",
    "        # Compute loss    \n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i < iterations):\n",
    "            # This is not the last iteration -> retain graph for next iteration\n",
    "            total_loss.backward(retain_graph=True)\n",
    "        else:\n",
    "            # This is the last iteration ... do not retain graph\n",
    "            total_loss.backward(retain_graph=False)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not(i % 100):\n",
    "            sys.stdout.write('.')\n",
    "\n",
    "    print()\n",
    "    print(f'Saving output image: {image_filename}')\n",
    "    # now save the resulting image in the output_images folder\n",
    "    convert_back_to_image(output_image, output_images + image_filename)\n",
    "    \n",
    "    del content_image\n",
    "    del style_image\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
